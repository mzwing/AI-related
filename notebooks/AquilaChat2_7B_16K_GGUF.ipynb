{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mzwing/AI-related/blob/master/notebooks/AquilaChat2_7B_16K_GGUF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lVceTkS1g9pR"
      },
      "outputs": [],
      "source": [
        "# huggingface\n",
        "!git config --global credential.helper store\n",
        "!huggingface-cli login\n",
        "# !huggingface-cli login --add-to-git-credential --token"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# clone repo\n",
        "!rm -rf sample_data\n",
        "!sudo apt-get install aria2 -y\n",
        "!GIT_LFS_SKIP_SMUDGE=1 git clone https://huggingface.co/mzwing/AquilaChat2-7B-16K-GGUF\n",
        "!rm -rf AquilaChat2-7B-16K-GGUF/AquilaChat2-7B-16K.F16.gguf\n",
        "!aria2c -c -x16 -d AquilaChat2-7B-16K-GGUF https://huggingface.co/mzwing/AquilaChat2-7B-16K-GGUF/resolve/main/AquilaChat2-7B-16K.F16.gguf?download=true -o AquilaChat2-7B-16K.F16.gguf"
      ],
      "metadata": {
        "id": "fbwFycgp79iv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get original model\n",
        "!sudo apt-get install aria2 -y\n",
        "!GIT_LFS_SKIP_SMUDGE=1 git clone https://huggingface.co/BAAI/AquilaChat2-7B-16K --depth 1\n",
        "!cd AquilaChat2-7B-16K && rm -rf pytorch_model-00001-of-00003.bin pytorch_model-00002-of-00003.bin pytorch_model-00003-of-00003.bin .git\n",
        "\n",
        "!echo -e \"https://huggingface.co/BAAI/AquilaChat2-7B-16K/resolve/main/pytorch_model-00001-of-00003.bin?download=true\\n out=pytorch_model-00001-of-00003.bin\\nhttps://huggingface.co/BAAI/AquilaChat2-7B-16K/resolve/main/pytorch_model-00002-of-00003.bin?download=true\\n out=pytorch_model-00002-of-00003.bin\\nhttps://huggingface.co/BAAI/AquilaChat2-7B-16K/resolve/main/pytorch_model-00003-of-00003.bin?download=true\\n out=pytorch_model-00003-of-00003.bin\" > download.txt\n",
        "!aria2c -c -x16 -d AquilaChat2-7B-16K --input-file=download.txt\n",
        "\n",
        "!rm -rf download.txt"
      ],
      "metadata": {
        "id": "_2RBV70CuKN4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert to fp32\n",
        "!pip3 install numpy sentencepiece transformers gguf protobuf\n",
        "!git clone https://github.com/ggerganov/llama.cpp --depth 1\n",
        "\n",
        "!mkdir -p AquilaChat2-7B-16K-GGUF\n",
        "\n",
        "!cd llama.cpp && python3 ./convert.py --outtype f16 --outfile ../AquilaChat2-7B-16K-GGUF/AquilaChat2-7B-16K.F32.gguf ../AquilaChat2-7B-16K/ --ctx 16384 --vocab-type hfft"
      ],
      "metadata": {
        "id": "6EBo9BBRueMq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DISoQb2bgA5c"
      },
      "outputs": [],
      "source": [
        "# convert to fp16\n",
        "!pip3 install numpy sentencepiece transformers gguf protobuf\n",
        "!git clone https://github.com/ggerganov/llama.cpp --depth 1\n",
        "\n",
        "!cd llama.cpp && python3 ./convert.py --outtype f16 --outfile ../AquilaChat2-7B-16K-GGUF/AquilaChat2-7B-16K.F16.gguf ../AquilaChat2-7B-16K/ --ctx 16384 --vocab-type hfft"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# uninstall to save space\n",
        "!pip3 uninstall numpy sentencepiece transformers gguf protobuf -y\n",
        "!rm -rf AquilaChat2-7B-16K llama.cpp"
      ],
      "metadata": {
        "id": "lmSurz1377Ye"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YGuBieGbnlRT"
      },
      "outputs": [],
      "source": [
        "# compile\n",
        "!cd llama.cpp && mkdir build && cd build && cmake .. -DLLAMA_BLAS=ON -DLLAMA_BLAS_VENDOR=OpenBLAS -DLLAMA_AVX2=ON -DCMAKE_BUILD_TYPE=Release && cmake --build . --config Release\n",
        "!cp llama.cpp/build/bin/quantize ./\n",
        "\n",
        "# store\n",
        "!mkdir -p /content/drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "!mkdir /content/drive/MyDrive/llama.cpp-cache/\n",
        "!cp quantize /content/drive/MyDrive/llama.cpp-cache/\n",
        "\n",
        "drive.flush_and_unmount()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b2bDPao1jam1"
      },
      "outputs": [],
      "source": [
        "# get compile result\n",
        "!mkdir -p /content/drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "!cp /content/drive/MyDrive/llama.cpp-cache/quantize ./\n",
        "\n",
        "drive.flush_and_unmount()\n",
        "\n",
        "!chmod +x quantize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7AdSBJlEPNyo"
      },
      "outputs": [],
      "source": [
        "# quantize 1\n",
        "import concurrent.futures\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "\n",
        "parameters = [\"Q8_0\", \"Q6_K\", \"Q5_K_M\", \"Q5_K_S\"]\n",
        "\n",
        "def run_command(param):\n",
        "    os.system(f\"cd AquilaChat2-7B-16K-GGUF/ && ../quantize AquilaChat2-7B-16K.F16.gguf AquilaChat2-7B-16K.{param}.gguf {param}\")\n",
        "\n",
        "# 使用ThreadPoolExecutor创建一个线程池，最大线程数为5\n",
        "with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n",
        "    # 利用map函数，将命令列表和函数进行匹配执行\n",
        "    list(tqdm(executor.map(run_command, parameters), total=len(parameters)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G1GZA1t3mHCY"
      },
      "outputs": [],
      "source": [
        "# quantize 2\n",
        "import concurrent.futures\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "\n",
        "parameters = [ \"Q5_0\", \"Q4_K_M\", \"Q4_K_S\", \"Q4_0\", \"Q3_K_L\", \"Q3_K_M\", \"Q3_K_S\", \"Q2_K\" ]\n",
        "\n",
        "def run_command(param):\n",
        "    os.system(f\"cd AquilaChat2-7B-16K-GGUF/ && ../quantize AquilaChat2-7B-16K.F16.gguf AquilaChat2-7B-16K.{param}.gguf {param}\")\n",
        "\n",
        "# 使用ThreadPoolExecutor创建一个线程池，最大线程数为5\n",
        "with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n",
        "    # 利用map函数，将命令列表和函数进行匹配执行\n",
        "    list(tqdm(executor.map(run_command, parameters), total=len(parameters)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "augC3gte-i4n"
      },
      "outputs": [],
      "source": [
        "# git lfs upload\n",
        "!git config --global user.email 'mzwing@mzwing.eu.org'\n",
        "!git config --global user.name 'mzwing'\n",
        "!cd AquilaChat2-7B-16K-GGUF && git lfs install\n",
        "!huggingface-cli lfs-enable-largefiles AquilaChat2-7B-16K-GGUF/\n",
        "!cd AquilaChat2-7B-16K-GGUF/ && git lfs track *.gguf\n",
        "!cd AquilaChat2-7B-16K-GGUF/ && git add .\n",
        "!cd AquilaChat2-7B-16K-GGUF/ && git commit -m \"GGUF model commit (made with llama.cpp commit 26d6076)\"\n",
        "!cd AquilaChat2-7B-16K-GGUF/ && git push"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mv AquilaChat2-7B-16K-GGUF/AquilaChat2-7B-16K.F16.gguf AquilaChat2-7B-16K-GGUF/AquilaChat2-7B-16K.F32.gguf"
      ],
      "metadata": {
        "id": "wOIPpXfr8J-N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# http upload\n",
        "import concurrent.futures\n",
        "from huggingface_hub import HfApi\n",
        "\n",
        "api = HfApi()\n",
        "\n",
        "# parameters = [\"Q5_0\", \"Q4_K_M\", \"Q4_K_S\", \"Q4_0\", \"Q3_K_L\", \"Q3_K_M\", \"Q3_K_S\", \"Q2_K\"]\n",
        "\n",
        "parameters = [ \"F32\" ]\n",
        "\n",
        "def upload(params):\n",
        "    print(f\"Uploading {params}...\")\n",
        "    api.upload_file(\n",
        "        path_or_fileobj=f\"AquilaChat2-7B-16K-GGUF/AquilaChat2-7B-16K.{params}.gguf\",\n",
        "        path_in_repo=f\"AquilaChat2-7B-16K.{params}.gguf\",\n",
        "        repo_id=\"mzwing/AquilaChat2-7B-16K-GGUF\",\n",
        "    )\n",
        "    print(f\"Finished uploading {params}.\")\n",
        "\n",
        "with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:\n",
        "    # Directly consume the iterator to ensure all futures are executed\n",
        "    list(executor.map(upload, parameters))"
      ],
      "metadata": {
        "id": "MY3wgOV464o3"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}