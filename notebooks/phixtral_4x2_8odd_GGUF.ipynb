{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mzwing/AI-related/blob/master/notebooks/phixtral_4x2_8odd_GGUF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prepare\n",
        "!rm -rf sample_data\n",
        "!mkdir -p phixtral-4x2_8odd-GGUF"
      ],
      "metadata": {
        "id": "jBxcmSgq7Bed"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get original model\n",
        "!sudo apt-get install aria2 -y\n",
        "!GIT_LFS_SKIP_SMUDGE=1 git clone https://huggingface.co/shadowml/phixtral-4x2_8odd --depth 1\n",
        "!cd phixtral-4x2_8odd && rm -rf model-00001-of-00001.safetensors .git\n",
        "\n",
        "!aria2c -c -x16 -d phixtral-4x2_8odd https://huggingface.co/shadowml/phixtral-4x2_8odd/resolve/main/model-00001-of-00001.safetensors?download=true -o model-00001-of-00001.safetensors"
      ],
      "metadata": {
        "id": "_2RBV70CuKN4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prepare for llama.cpp quantise\n",
        "!pip3 install sentencepiece gguf\n",
        "!mkdir -p phixtral-4x2_8odd\n",
        "\n",
        "!git clone https://github.com/ggerganov/llama.cpp -b gg/add-phixtral --depth 1"
      ],
      "metadata": {
        "id": "8BKYAQ4RKklg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Here you should use your own convert-hf-to-gguf.py to fix the bug about model slicing\n",
        "!rm -rf llama.cpp/convert-hf-to-gguf.py\n",
        "!mv convert-hf-to-gguf.py llama.cpp/"
      ],
      "metadata": {
        "id": "RDUi4vq6mssi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert to fp32\n",
        "!cd llama.cpp && python3 ./convert-hf-to-gguf.py --outtype f32 --outfile ../phixtral-4x2_8odd-GGUF/phixtral-4x2_8odd.F32.gguf ../phixtral-4x2_8odd/"
      ],
      "metadata": {
        "id": "6EBo9BBRueMq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DISoQb2bgA5c"
      },
      "outputs": [],
      "source": [
        "# convert to fp16\n",
        "!cd llama.cpp && python3 ./convert-hf-to-gguf.py --outtype f16 --outfile ../phixtral-4x2_8odd-GGUF/phixtral-4x2_8odd.F16.gguf ../phixtral-4x2_8odd/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# uninstall to save space\n",
        "!pip3 uninstall sentencepiece gguf -y\n",
        "!pip cache purge\n",
        "!rm -rf phixtral-4x2_8odd llama.cpp"
      ],
      "metadata": {
        "id": "lmSurz1377Ye"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prepare for CPU\n",
        "!wget -O- https://apt.repos.intel.com/intel-gpg-keys/GPG-PUB-KEY-INTEL-SW-PRODUCTS.PUB | gpg --dearmor | sudo tee /usr/share/keyrings/oneapi-archive-keyring.gpg > /dev/null\n",
        "!echo \"deb [signed-by=/usr/share/keyrings/oneapi-archive-keyring.gpg] https://apt.repos.intel.com/oneapi all main\" | sudo tee /etc/apt/sources.list.d/oneAPI.list\n",
        "!sudo apt-get update\n",
        "!sudo apt-get install intel-oneapi-mkl -y"
      ],
      "metadata": {
        "id": "aXF9nZXV5vv1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b2bDPao1jam1"
      },
      "outputs": [],
      "source": [
        "# get compile result (CPU)\n",
        "!aria2c -c -x16 https://github.com/MZWNET/actions/releases/download/llama_cpp-phixtral-9998ecd/llama-phixtral-9998ecd-bin-linux-avx2-intel-mkl-x64.zip\n",
        "!mkdir -p llama.cpp-bin\n",
        "!unzip llama-phixtral-9998ecd-bin-linux-avx2-intel-mkl-x64.zip -d llama.cpp-bin\n",
        "!mv -f llama.cpp-bin/main .\n",
        "!mv -f llama.cpp-bin/quantize .\n",
        "!rm -rf llama.cpp-bin llama-phixtral-9998ecd-bin-linux-avx2-intel-mkl-x64.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p phixtral-4x2_8odd-GGUF\n",
        "!aria2c -c -x16 -d phixtral-4x2_8odd-GGUF https://huggingface.co/mzwing/phixtral-4x2_8odd-GGUF/resolve/main/phixtral-4x2_8odd.Q2_K.gguf?download=true -o phixtral-4x2_8odd.Q2_K.gguf"
      ],
      "metadata": {
        "id": "1JBxTFVl4cFj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prepare for GPU\n",
        "!sudo apt-get install nvidia-cuda-toolkit -y"
      ],
      "metadata": {
        "id": "1QLXZbjO6Hde"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get compile result (GPU)\n",
        "!aria2c -c -x16 https://github.com/MZWNET/actions/releases/download/llama_cpp-phixtral-9998ecd/llama-phixtral-9998ecd-bin-linux-avx2-cublas-cu121-x64.zip\n",
        "!mkdir -p llama.cpp-bin\n",
        "!unzip llama-phixtral-9998ecd-bin-linux-avx2-cublas-cu121-x64.zip -d llama.cpp-bin\n",
        "!mv -f llama.cpp-bin/main .\n",
        "!mv -f llama.cpp-bin/quantize .\n",
        "!rm -rf llama.cpp-bin llama-phixtral-9998ecd-bin-linux-avx2-cublas-cu121-x64.zip"
      ],
      "metadata": {
        "id": "NVboRtai6Kgx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7AdSBJlEPNyo"
      },
      "outputs": [],
      "source": [
        "# quantize\n",
        "import concurrent.futures\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "\n",
        "#parameters = [ \"Q8_0\", \"Q6_K\", \"Q5_K_M\", \"Q5_K_S\", \"Q5_0\", \"Q4_K_M\", \"Q4_K_S\", \"Q4_0\", \"Q3_K_L\", \"Q3_K_M\", \"Q3_K_S\", \"Q2_K\" ]\n",
        "parameters = [ \"Q5_K_S\", \"Q5_0\", \"Q4_K_M\", \"Q4_K_S\", \"Q4_0\", \"Q3_K_L\", \"Q3_K_M\", \"Q3_K_S\", \"Q2_K\" ]\n",
        "\n",
        "def run_command(param):\n",
        "    os.system(f\"cd phixtral-4x2_8odd-GGUF/ && ../quantize phixtral-4x2_8odd.F16.gguf phixtral-4x2_8odd.{param}.gguf {param}\")\n",
        "\n",
        "with concurrent.futures.ThreadPoolExecutor(max_workers=12) as executor:\n",
        "    list(tqdm(executor.map(run_command, parameters), total=len(parameters)))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# http upload\n",
        "import concurrent.futures\n",
        "from huggingface_hub import HfApi, create_repo\n",
        "\n",
        "# create repo\n",
        "create_repo(\"mzwing/phixtral-4x2_8odd-GGUF\")\n",
        "\n",
        "api = HfApi()\n",
        "\n",
        "parameters = [ \"F16\", \"Q8_0\", \"Q6_K\", \"Q5_K_M\", \"Q5_K_S\", \"Q5_0\", \"Q4_K_M\", \"Q4_K_S\", \"Q4_0\", \"Q3_K_L\", \"Q3_K_M\", \"Q3_K_S\", \"Q2_K\" ]\n",
        "\n",
        "def upload(params):\n",
        "    api.upload_file(\n",
        "        path_or_fileobj=f\"phixtral-4x2_8odd-GGUF/phixtral-4x2_8odd.{params}.gguf\",\n",
        "        path_in_repo=f\"phixtral-4x2_8odd.{params}.gguf\",\n",
        "        repo_id=\"mzwing/phixtral-4x2_8odd-GGUF\",\n",
        "    )\n",
        "\n",
        "with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:\n",
        "    # Directly consume the iterator to ensure all futures are executed\n",
        "    list(executor.map(upload, parameters))"
      ],
      "metadata": {
        "id": "MY3wgOV464o3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# git merge history\n",
        "!git config --global credential.helper store\n",
        "!huggingface-cli login\n",
        "!git config --global user.email 'mzwing@mzwing.eu.org'\n",
        "!git config --global user.name 'mzwing'\n",
        "!rm -rf phixtral-4x2_8odd-GGUF/\n",
        "!GIT_LFS_SKIP_SMUDGE=1 git clone https://huggingface.co/mzwing/phixtral-4x2_8odd-GGUF\n",
        "!cd phixtral-4x2_8odd-GGUF && git lfs install\n",
        "!huggingface-cli lfs-enable-largefiles phixtral-4x2_8odd-GGUF/\n",
        "!cd phixtral-4x2_8odd-GGUF/ && git branch backup-main\n",
        "!cd phixtral-4x2_8odd-GGUF/ && git checkout --orphan new-main\n",
        "!cd phixtral-4x2_8odd-GGUF/ && git add -A\n",
        "!cd phixtral-4x2_8odd-GGUF/ && git commit -m \"GGUF model commit (made with llama.cpp commit 9998ecd)\"\n",
        "!cd phixtral-4x2_8odd-GGUF/ && git branch -D main\n",
        "!cd phixtral-4x2_8odd-GGUF/ && git branch -m main\n",
        "!cd phixtral-4x2_8odd-GGUF/ && git push -f origin main"
      ],
      "metadata": {
        "id": "lMPDqJ56PfLH"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}